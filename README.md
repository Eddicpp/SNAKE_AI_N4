# ğŸ Snake AI - Genetic Algorithm Training

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Pygame](https://img.shields.io/badge/pygame-2.0+-green.svg)
![NumPy](https://img.shields.io/badge/numpy-1.20+-orange.svg)


https://github.com/user-attachments/assets/377e8d9e-8a85-45d5-9997-785b43573542


**Progetto di Machine Learning:** Addestramento di un agente Snake usando **Algoritmi Genetici** (Genetic Algorithm - GA) per imparare a giocare autonomamente.

---

## ğŸ“‹ Indice

1. [Panoramica](#-panoramica)
2. [Come Funziona](#-come-funziona)
3. [Architettura](#-architettura)
4. [Installazione](#-installazione)
5. [Utilizzo](#-utilizzo)
6. [Parametri Configurabili](#ï¸-parametri-configurabili)
7. [Reward Function](#-reward-function)
8. [Limitazioni degli Algoritmi Genetici](#ï¸-limitazioni-degli-algoritmi-genetici)
9. [Risultati Attesi](#-risultati-attesi)
10. [Miglioramenti Futuri](#-miglioramenti-futuri)
11. [Contribuire](#-contribuire)

---

## ğŸ¯ Panoramica

Questo progetto implementa un sistema di **apprendimento evolutivo** per il gioco Snake classico. Una popolazione di agenti (snake) evolve attraverso generazioni successive, migliorando progressivamente le proprie capacitÃ  di gioco attraverso:

- **Selezione Naturale:** Sopravvivono solo gli snake migliori
- **Crossover:** Combinazione dei "geni" (pesi neurali) degli snake di Ã©lite
- **Mutazione:** Variazioni casuali per esplorare nuove strategie

### ğŸ® Demo Progressione
```
Generazione 1:    Avg Score: 1.2 mele  (movimento casuale)
Generazione 100:  Avg Score: 8.5 mele  (segue la mela)
Generazione 500:  Avg Score: 15.3 mele (evita ostacoli)
Generazione 1000: Avg Score: 22.7 mele (strategie avanzate)
```

---

## ğŸ§  Come Funziona

### **Algoritmo Genetico (GA)**

A differenza dei metodi di Reinforcement Learning classici (Q-Learning, DQN), gli **Algoritmi Genetici** non aggiornano i pesi della rete neurale dopo ogni azione. Invece:

1. **Generazione iniziale:** Crea N snake con pesi neurali casuali
2. **Valutazione:** Ogni snake gioca una partita completa fino alla morte
3. **Fitness Score:** Calcola il punteggio basato su:
   - Mele mangiate
   - Avvicinamento/allontanamento dalla mela
   - Esplorazione vs loop
   - Tempo di sopravvivenza
4. **Selezione:** Seleziona il **top 10%** (Ã©lite)
5. **Riproduzione:** 
   - Mantieni Ã©lite invariata (10%)
   - Genera il restante 90% attraverso **crossover** di due genitori casuali dall'Ã©lite
   - Applica **mutazione** (modifiche casuali ai pesi)
6. **Nuova generazione:** Ripeti da step 2

### **Diagramma Flusso**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Population  â”‚  100 snakes con pesi casuali
â”‚   Gen 1     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Ogni snake gioca fino alla morte
       â”‚ Calcola fitness per ognuno
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Valutazione â”‚  Ordina per fitness
â”‚  & Ranking   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Selezione   â”‚  Seleziona top 10 snake (Ã©lite)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–º 10%: Ã‰lite (copia identica)
       â”‚
       â”œâ”€â–º 90%: Crossover (combina pesi di 2 genitori)
       â”‚        + Mutazione (modifiche casuali)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Population  â”‚  Nuovi 100 snakes
â”‚   Gen 2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PerchÃ© Funziona?**

- **Selezione:** Gli snake migliori trasmettono i loro "geni" (pesi) alle generazioni future
- **Crossover:** Combina strategie vincenti di genitori diversi
- **Mutazione:** Esplora nuove soluzioni, evita minimi locali
- **Evoluzione:** Dopo molte generazioni, la popolazione converge verso strategie ottimali

---

## ğŸ—ï¸ Architettura

### **Struttura File**
```
snake/
â”œâ”€â”€ main.py              # Loop di training principale
â”œâ”€â”€ backSnake.py         # Rete neurale (cervello snake)
â”œâ”€â”€ snakeGame.py         # Ambiente di gioco Snake
â”œâ”€â”€ saver.py             # Sistema salvataggio/caricamento checkpoint
â”œâ”€â”€ cleanup.py           # Utility pulizia checkpoint incompatibili
â”œâ”€â”€ README.md            # Questo file
â”œâ”€â”€ saved_models/        # Directory checkpoint automatici
â”‚   â”œâ”€â”€ checkpoint_gen_50.npz
â”‚   â”œâ”€â”€ checkpoint_gen_100.npz
â”‚   â”œâ”€â”€ best_snake_gen_100.npz
â”‚   â”œâ”€â”€ best_snake_overall.npz
â”‚   â””â”€â”€ training_history.json
â””â”€â”€ logs/
    â””â”€â”€ training_log.txt
```

### **Rete Neurale (6 Layer Deep Network)**
```
Input Layer:  24 neuroni (vision raycasting 8 direzioni Ã— 3 info)
    â†“
Layer 1:      24 â†’ 24 neuroni (Tanh activation)
    â†“
Layer 2:      24 â†’ 20 neuroni (Tanh)
    â†“
Layer 3:      20 â†’ 16 neuroni (Tanh)
    â†“
Layer 4:      16 â†’ 12 neuroni (Tanh)
    â†“
Layer 5:      12 â†’ 8 neuroni (Tanh)
    â†“
Output Layer: 8 â†’ 4 neuroni (Softmax)

Totale parametri: ~5,000 pesi
```

**Funzioni di attivazione:**
- Hidden layers: `Tanh` (range -1 a 1, utile per GA)
- Output layer: `Softmax` (converte in probabilitÃ  per 4 azioni)

### **Input (Vision System - Raycasting)**

Lo snake "vede" in **8 direzioni** (N, NE, E, SE, S, SW, W, NW) usando raycasting.

Per ogni direzione, la rete riceve 3 valori:
```python
[
    distanza_mela,     # 0.0-1.0 (1/distanza, 0 se non visibile)
    distanza_corpo,    # 0.0-1.0 (1/distanza, 0 se non visibile)
    distanza_muro      # 0.0-1.0 (1/distanza)
]
```

**Totale input:** 8 direzioni Ã— 3 info = **24 valori**

**Esempio visivo:**
```
     N
  NW â”‚ NE
W â”€â”€â”€ğŸâ”€â”€â”€ E
  SW â”‚ SE
     S

Ogni raggio "lancia" un fascio fino al muro,
rilevando mela/corpo lungo il percorso
```

### **Output (4 Azioni)**
```python
Output[0] â†’ UP     (ProbabilitÃ : 0.1)
Output[1] â†’ DOWN   (ProbabilitÃ : 0.2)
Output[2] â†’ LEFT   (ProbabilitÃ : 0.5) â† Scelta!
Output[3] â†’ RIGHT  (ProbabilitÃ : 0.2)

Total: 1.0 (softmax normalizzazione)
```

L'azione con **probabilitÃ  massima** viene eseguita.

---

## ğŸš€ Installazione

### **Requisiti**

- Python 3.8 o superiore
- pip (package manager)

### **Setup Rapido**
```bash
# 1. Clone repository
git clone https://github.com/tuo-username/snake-ai-genetic.git
cd snake-ai-genetic

# 2. Installa dipendenze
pip install numpy pygame

# 3. Verifica installazione
python -c "import numpy, pygame; print('âœ… Setup completato!')"
```

### **Dipendenze Dettagliate**
```
numpy>=1.20.0     # Calcoli matrici e operazioni neurali
pygame>=2.0.0     # Rendering grafico e game loop
```

---

## ğŸ’» Utilizzo

### **1. Avvio Training da Zero**
```bash
python main.py
```

**Output iniziale:**
```
ğŸ’¾ SnakeSaver inizializzato in: ./
âš ï¸ Nessuna history trovata

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ†• NUOVO TRAINING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ SNAKE AI TRAINING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Generazione start: 0
Generazione target: 1000000
Popolazione: 1000
Mutation rate: 1.000000
ModalitÃ : âš¡ VELOCE (no render)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âŒ¨ï¸  CONTROLLI:
  V - Toggle fast/visual mode
  S - Salva ora
  ESC - Esci e salva
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GENERAZIONE 1/1000000
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ† Best: Score=3, Fitness=25847.0 | Avg: 1.23
```

### **2. Resume Training (Riprendi da Checkpoint)**

Se hai giÃ  checkpoint salvati:
```bash
python main.py
```

**Output con checkpoint esistenti:**
```
ğŸ” CHECKPOINT TROVATI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ Checkpoint disponibili:
  Gen   50 | Î¼: 0.778801 | 2024-12-27 14:23:45
  Gen  100 | Î¼: 0.605006 | 2024-12-27 15:10:22
  Gen  150 | Î¼: 0.470348 | 2024-12-27 16:05:11

ğŸ’¾ Checkpoint piÃ¹ recente: Gen 150

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Vuoi continuare il training esistente?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  [R] - Resume da Gen 150
  [N] - Nuovo training (cancella progressi)
  [C] - Scegli checkpoint specifico
  [ESC] - Esci
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Premi tasto...
```

**Opzioni:**
- **R**: Riprende dal checkpoint piÃ¹ recente
- **N**: Cancella tutto e riparte da Gen 1 (chiede conferma)
- **C**: Scegli manualmente quale checkpoint caricare
- **ESC**: Esci senza fare nulla

### **3. Pulizia Checkpoint Incompatibili**

Dopo upgrade architettura (es. 4â†’6 layer), i checkpoint vecchi non funzionano:
```bash
python cleanup.py
```

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§¹ CLEANUP CHECKPOINT VECCHI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ Checkpoint trovati:
  âš ï¸  Gen  50 | 4-layer (OLD)   | 2024-12-26 10:30:45
  âš ï¸  Gen 100 | 4-layer (OLD)   | 2024-12-26 11:15:22
  âœ… Gen 150 | 6-layer         | 2024-12-27 14:00:00

Totale: 3 checkpoint
  - Vecchi (4-layer): 2
  - Nuovi (6-layer):  1

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸  ATTENZIONE: Trovati checkpoint incompatibili
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
I checkpoint vecchi (4-layer) non sono compatibili con
l'architettura attuale (6-layer) e causeranno errori.

Vuoi eliminarli? [s/N]: s

ğŸ—‘ï¸  Eliminazione in corso...
  ğŸ—‘ï¸  checkpoint_gen_50.npz
  ğŸ—‘ï¸  checkpoint_gen_100.npz

âœ… Eliminati 2 checkpoint vecchi
```

### **4. Controlli Durante Training**

| Tasto | Azione | Descrizione |
|-------|--------|-------------|
| **V** | Toggle Visual/Fast Mode | Mostra/nasconde rendering |
| **S** | Save Now | Salvataggio manuale immediato |
| **ESC** | Exit & Save | Salva e chiudi training |

### **5. ModalitÃ  Visual (Debug)**

Premi **V** durante il training per attivare visualizzazione:
```
ModalitÃ : ğŸ¬ VISUAL (con render)
```

**Cosa vedi:**
- Griglia 20Ã—20 con snake e mela
- Testa verde chiaro, corpo verde scuro
- Mela rossa
- Info: Generazione, Fitness, Score

**âš ï¸ Attenzione:** ModalitÃ  visual Ã¨ ~10Ã— piÃ¹ lenta! Usa solo per debugging.

---

## âš™ï¸ Parametri Configurabili

### **In `main.py`:**
```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAZIONE TRAINING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_GENERATIONS = 1000000    # Generazioni totali
POPULATION_SIZE = 1000       # Snake per generazione

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAZIONE MODALITÃ€ VISUALIZZAZIONE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FAST_MODE = True             # True = veloce, False = visual debug
RENDER_EVERY_N_GENS = 1      # Mostra render ogni N generazioni

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAZIONE SALVATAGGIO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SAVE_BEST_EVERY = 10         # Salva best snake ogni N generazioni
SAVE_CHECKPOINT_EVERY = 50   # Checkpoint popolazione ogni N generazioni

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAZIONE TIMEOUT DINAMICO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASE_TIMEOUT = 2000000       # Step massimi senza mangiare mela
TIMEOUT_INCREMENT = 0        # +N step per ogni generazione
```

### **Mutation Rate (Decay Automatico)**

Il mutation rate decresce automaticamente per bilanciare exploration vs exploitation:
```python
# In main.py
epsilon = 0.995              # Decay factor (moltiplicatore)
min_rate = 0.05              # Rate minimo (floor)

# Formula: rate = max(rate Ã— epsilon, min_rate)
```

**Progressione tipica:**

| Generazione | Mutation Rate | Fase |
|-------------|---------------|------|
| 1 | 1.0 (100%) | ğŸ” Exploration massima |
| 100 | 0.60 (60%) | ğŸ” Exploration alta |
| 500 | 0.08 (8%) | âš–ï¸ Balance exploration/exploitation |
| 1000 | 0.05 (5%) | ğŸ¯ Exploitation (minimo) |
| 2000+ | 0.05 (5%) | ğŸ¯ Fine-tuning |

### **Grid Size & Game Settings**

In `snakeGame.py`:
```python
CELL_SIZE = 20      # Pixel per cella
GRID_WIDTH = 20     # Larghezza griglia
GRID_HEIGHT = 20    # Altezza griglia

# Totale celle: 20 Ã— 20 = 400 celle
# Snake completo = 400 mele (impossibile con GA!)
```

---

## ğŸ Reward Function

La reward function Ã¨ **il cuore dell'apprendimento**. Guida lo snake verso comportamenti desiderati.

### **Componenti Reward (in ordine di importanza)**

| Evento | Reward | Moltiplicatore | Note |
|--------|--------|----------------|------|
| **Mela mangiata (centro)** | +10,000 | 1Ã— | Base reward |
| **Mela mangiata (bordo)** | +30,000 - 60,000 | 3Ã— | PiÃ¹ difficile raggiungere bordi |
| **Mela mangiata (angolo)** | +110,000 | 8Ã— | Molto difficile, massimo reward |
| **Avvicinamento mela** | +100 per unitÃ  | Centro: 1Ã—<br>Bordo: 3Ã—<br>Angolo: 8Ã— | Gradient verso mela |
| **Allontanamento mela** | -50 per unitÃ  | Centro: 1Ã—<br>Bordo: 2Ã—<br>Angolo: 4Ã— | PenalitÃ  allontanamento |
| **Vicinanza mela (<3 celle)** | +400-1200 | Angolo: 3Ã—<br>Bordo: 1.5Ã— | Incentiva vicinanza |
| **Esplorazione (nuova cella)** | +5 | - | Bonus esplorazione |
| **Revisita cella (2Â° volta)** | -40 | - | PenalitÃ  revisita |
| **Revisita cella (3+ volte)** | -20,000,000 + **GAME OVER** | - | **Loop detection** |
| **Corpo molto vicino (<1 cella)** | -5,000 | - | Evita auto-collisione imminente |
| **Corpo vicino (1-2 celle)** | -2,000 | - | Warning body proximity |
| **Corpo medio (2-3 celle)** | -500 | - | Soft penalty |
| **Timeout (no mela)** | -0.1 Ã— step^1.5 | - | PenalitÃ  crescente tempo perso |
| **Sopravvivenza per step** | -0.5 | - | Pressione temporale costante |
| **Morte (collisione)** | -5,000 | - | PenalitÃ  terminale |

### **Reward Speciali: Corner vs Edge vs Center**

La reward function **distingue automaticamente** la difficoltÃ  in base alla posizione della mela:

**Corner (Angolo - 2 muri adiacenti):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ğŸ        â”‚  â† Corner top-left
â”‚          â”‚
â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reward approaching: Ã—8
Reward eating:      +110,000
Timeout:            150 step
```

**Edge (Bordo - 1 muro vicino):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          â”‚
â”‚ğŸ         â”‚  â† Edge left
â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reward approaching: Ã—3
Reward eating:      +30,000 - 60,000
Timeout:            120 step
```

**Center (Centro - nessun muro):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          â”‚
â”‚    ğŸ     â”‚  â† Center
â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reward approaching: Ã—1
Reward eating:      +10,000
Timeout:            100 step
```

### **Loop Detection (Anti-Circling)**

Sistema automatico che **termina immediatamente** la partita se lo snake gira in loop:
```python
# Tracking posizioni visitate
if position visited >= 3 times:
    fitness -= 20,000,000  # PenalitÃ  devastante
    game.dead = True       # Game over immediato
```

**Reset:** Le posizioni visitate si resettano quando mangia una mela.

### **Esempio Completo Calcolo Fitness**
```python
# Snake gioca per 150 step, mangia 3 mele

Mela 1 (centro):       +10,000
  Avvicinamento (Ã—20): +2,000
  
Mela 2 (bordo):        +45,000
  Avvicinamento (Ã—15): +4,500
  
Mela 3 (angolo):       +110,000
  Avvicinamento (Ã—10): +8,000
  
Esplorazione (40 celle): +200
Revisita (10 celle):     -200
Sopravvivenza (150 step): -75

FITNESS TOTALE: 179,425
```

---

## âš ï¸ Limitazioni degli Algoritmi Genetici

### **1. ğŸ“Š ScalabilitÃ  Limitata (Max ~30-50 mele su 20Ã—20)**

**Problema:**
- Snake 20Ã—20 completo richiede **400 celle** riempite
- Con GA puro: realisticamente arrivi a **25-40 mele** massimo
- Oltre questo punto, il miglioramento diventa **estremamente lento**

**Motivo tecnico:**
- GA ottimizza per "prossima mela immediata", non per strategie a lungo termine
- PiÃ¹ lo snake cresce, piÃ¹ lo spazio di ricerca esplode esponenzialmente: **4^N** azioni possibili
- Reward troppo sparsi nel tempo: mela #1 e mela #50 ricevono stesso reward base (+10k)

**Risultati Realistici:**
```
Gen 100:   ~8-12 mele   âœ… GiÃ  ottimo per GA!
Gen 500:   ~15-25 mele  âœ… Molto buono
Gen 2000:  ~25-35 mele  âœ… Eccellente per GA
Gen 5000+: ~35-50 mele  âš ï¸  Limite pratico GA
Gen 10000: ~40-55 mele  âŒ Rendimenti decrescenti
```

**Confronto algoritmi (per 400 mele - snake completo):**

| Algoritmo | ProbabilitÃ  successo | Tempo training |
|-----------|---------------------|----------------|
| **GA (questo progetto)** | âŒ ~0% | Impossibile |
| **Deep Q-Learning (DQN)** | âš ï¸ ~10-20% | Settimane |
| **PPO/A3C** | âš ï¸ ~30-50% | Settimane-Mesi |
| **AlphaZero + MCTS** | âœ… ~90%+ | Mesi |
| **Hamiltonian Path** | âœ… 100% | Istantaneo (ma non Ã¨ ML) |

### **2. ğŸ§  No Memory tra Generazioni**

**Problema:**
- Ogni snake riparte da zero ogni episodio
- Non accumula "esperienza graduale" come Reinforcement Learning
- Deve riscoprire strategie ogni volta

**Confronto RL vs GA:**
```python
# âœ… Reinforcement Learning (DQN):
for step in range(1_000_000):
    action = model.predict(state)
    reward = env.step(action)
    model.update(state, action, reward)  # â† Impara gradualmente ogni step
    
# âŒ Genetic Algorithm (GA):
for generation in range(1000):
    for snake in population:
        snake.play_full_game()  # â† No update durante gioco!
    select_best_and_evolve()    # â† Impara solo alla fine generazione
```

**Conseguenza:**
- RL impara da **ogni singola azione** (1M sample = 1M learning update)
- GA impara da **ogni generazione completa** (1000 gen = 1000 learning update)
- **Sample efficiency:** RL Ã¨ ~1000Ã— piÃ¹ efficiente

### **3. ğŸ¯ Reward Shaping Estremamente Critico**

**Problema:**
- GA Ã¨ **ipersensibile** al bilanciamento della reward function
- Se reward Ã¨ mal progettata â†’ snake impara strategie "pigre" (local minima)
- Non c'Ã¨ "safety net" come in RL (epsilon-greedy exploration, ecc.)

**Esempio SBAGLIATO che causa loop infinito:**
```python
# âŒ BAD REWARD DESIGN:
if approaching_apple:
    reward += 1000  # Troppo alta rispetto a eating!
    
if ate_apple:
    reward += 100   # Troppo bassa!

# Risultato disastroso:
# Snake gira in tondo vicino alla mela:
#   100 step Ã— 1000 reward = 100,000 punti
# 
# Mangiare la mela una volta:
#   1 Ã— 100 reward = 100 punti
#
# Strategia ottimale per GA: GIRA IN TONDO FOREVER ğŸ”„
```

**Fix corretto:**
```python
# âœ… GOOD REWARD DESIGN:
if approaching_apple:
    reward += 100   # Moderato
    
if ate_apple:
    reward += 10,000  # 100Ã— piÃ¹ importante!

# Ora mangiare Ã¨ 100Ã— piÃ¹ vantaggioso che avvicinarsi
```

### **4. ğŸ”® No Long-Term Planning**

**Problema:**
- GA prende decisioni "greedy" (miopi) basate solo su stato corrente
- Non pianifica conseguenze a 10-20 mosse nel futuro
- Non "pensa": "Se vado qui ora, tra 15 mosse sarÃ² intrappolato"

**Esempio visivo del problema:**
```
Snake lungo (30 celle) vede mela:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸğŸğŸğŸğŸğŸğŸğŸğŸâ”‚
  â”‚ ğŸ            ğŸâ”‚
  â”‚ ğŸ    ğŸ      ğŸâ”‚
  â”‚ ğŸ            ğŸâ”‚
  â”‚ ğŸğŸğŸğŸğŸğŸğŸğŸğŸâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GA Decision (greedy): 
  "Mela Ã¨ al centro â†’ vai dritto!"
  
Risultato:
  ğŸ’€ Si intrappola nel proprio corpo dopo 5 mosse

Decisione ottimale (richiede planning):
  "Aspetta che il corpo liberi il passaggio"
  "Segui percorso hamiltoniano sicuro"
  â†‘ Questo richiede reasoning multi-step che GA non ha
```

**Soluzione per long-term planning:** Algoritmi come AlphaZero con Monte Carlo Tree Search (MCTS).

### **5. âš–ï¸ Exploration vs Exploitation Trade-off**

**Problema:**
- Mutation rate decresce nel tempo (epsilon decay: 0.995^gen)
- Generazioni iniziali: troppa randomness â†’ inefficiente
- Generazioni tarde: troppo poca exploration â†’ stuck in local minima

**Progressione mutation rate:**
```python
Gen 1:    Mutation rate = 1.0   (100% randomness)
          â†“ Troppo casuale, perde tempo
          
Gen 100:  Mutation rate = 0.60  (60% exploration)
          â†“ Buon balance
          
Gen 500:  Mutation rate = 0.08  (8% exploration)
          â†“ Quasi convergenza
          
Gen 1000: Mutation rate = 0.05  (5% exploration - MINIMO)
          â†“ Popolazione converge su strategia locale
          
Gen 2000: Mutation rate = 0.05  (locked al minimo)
          âŒ Se esiste strategia migliore altrove â†’ NON la scoprirÃ  mai
```

**Problematica del local minimum:**
```
Fitness landscape (semplificato):

   ^
   â”‚    ğŸ”ï¸ (Global Maximum)
   â”‚   /  \          Best strategy: 50 mele
F  â”‚  /    \
i  â”‚ /      \
t  â”‚/        \___  
n  â”‚             â›°ï¸ (Local Maximum)
e  â”‚            /  \   GA converge qui: 35 mele
s  â”‚           /    \
s  â”‚__________/______\______________>
                   Strategies
```

**Conseguenza:** Una volta che popolazione converge (Gen 1000+), Ã¨ quasi impossibile "saltare" a un massimo globale lontano.

### **6. ğŸ“‰ Sample Inefficiency Massiva**

**Problema:**
- GA richiede **milioni di episodi** per performance decenti
- Ogni generazione = popolazione intera gioca (1000 snakes Ã— 1000 gen = 1,000,000 partite)
- Altri algoritmi raggiungono stesso livello con 100Ã— meno sample

**Confronto sample efficiency:**

| Algoritmo | Sample per 30 mele | Training time | Motivo inefficienza |
|-----------|-------------------|---------------|---------------------|
| **GA (questo)** | 500K-1M episodi | Ore-Giorni | Ogni snake riparte da zero |
| **DQN** | 50K-100K step | Giorni | Replay buffer riutilizza esperienze |
| **PPO** | 10K-50K step | Giorni-Settimane | Policy gradient + advantage |
| **AlphaZero** | 1K-10K episodi | Settimane-Mesi | Self-play + MCTS planning |

**Calcolo costo computazionale (esempio 1000 generazioni):**
```
1000 generazioni Ã— 1000 snakes Ã— 200 step/snake (media)
= 200,000,000 step totali
= ~5-10 ore su CPU moderna

DQN per stesso risultato:
50,000 step Ã— update ogni step
= ~1-2 ore
```

### **7. ğŸ² Nessuna Garanzia di Convergenza**

**Problema:**
- GA Ã¨ **stocastico**: due training run diversi â†’ risultati molto diversi
- Non c'Ã¨ garanzia matematica di convergenza all'ottimo
- "Lucky seeds" possono fare grande differenza

**Varianza tra run:**
```
Run 1 (seed=42):   Gen 1000 â†’ 38 mele âœ…
Run 2 (seed=123):  Gen 1000 â†’ 25 mele âš ï¸
Run 3 (seed=999):  Gen 1000 â†’ 42 mele âœ…âœ…

Stessi parametri, risultati diversi!
```

**Best practice:** Fai 3-5 training run con seed diversi, scegli il migliore.

---

## ğŸ† Risultati Attesi

### **Progressione Tipica (Population 1000, Default Settings)**

| Generazione | Best Score | Avg Score | Std Dev | Comportamento Osservato |
|-------------|-----------|-----------|---------|-------------------------|
| **1-10** | 1-3 mele | 0.5 | Â±0.8 | Movimento completamente casuale, molte collisioni immediate |
| **20-50** | 5-8 mele | 2-3 | Â±2.1 | Inizia a evitare muri, segue mela in linea retta |
| **50-100** | 10-15 mele | 5-7 | Â±3.5 | Segue mela costantemente, evita corpo in situazioni semplici |
| **100-300** | 15-20 mele | 8-12 | Â±4.2 | Strategie per bordi, inizia a pianificare 2-3 mosse avanti |
| **300-500** | 20-28 mele | 12-16 | Â±5.1 | Evita loop efficacemente, va verso angoli per mele high-reward |
| **500-1000** | 25-35 mele | 15-20 | Â±5.8 | Performance ottimizzata per GA, strategie mature |
| **1000-2000** | 30-40 mele | 18-25 | Â±6.2 | Fine-tuning, miglioramenti marginali |
| **2000+** | 35-50 mele | 20-28 | Â±6.5 | âš ï¸ Limite pratico GA, plateau |

### **Distribuzione Score (Gen 1000 tipica)**
```
ğŸ“Š DISTRIBUZIONE SCORE:
  0+ mele: 1000 snakes (100.0%)
  1+ mele:  987 snakes ( 98.7%)
  3+ mele:  856 snakes ( 85.6%)
  5+ mele:  723 snakes ( 72.3%)
  8+ mele:  512 snakes ( 51.2%)
 10+ mele:  345 snakes ( 34.5%)
 12+ mele:  198 snakes ( 19.8%)
 15+ mele:   87 snakes (  8.7%)
 20+ mele:   23 snakes (  2.3%)
 25+ mele:    5 snakes (  0.5%)
 30+ mele:    1 snake  (  0.1%) â† Best!
```

### **Record Personale Atteso**

Con parametri default e 1000+ generazioni di training:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ† BEST SNAKE OVERALL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Generation:     1247
Best Score:     59 mele (su 400 possibili = 9.5%)
Fitness:        2,847,293
Avg per episodio: 22.7 mele
Mutation rate:  0.05 (minimo)
Timestamp:      2024-12-27 18:45:32
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### **Metriche di Salute Training**

**1. Diversity Score (Top 10)**
```python
# Gen 100:
Top 10 scores: [15, 14, 14, 13, 12, 12, 11, 11, 10, 10]
Unique: 6/10  âœ… Buona diversity

# Gen 1000:
Top 10 scores: [35, 33, 32, 32, 31, 30, 30, 29, 29, 28]
Unique: 8/10  âœ… Ottima diversity!

# Gen 2000 (problema):
Top 10 scores: [38, 38, 38, 37, 37, 37, 37, 36, 36, 36]
Unique: 3/10  âš ï¸  Convergenza eccessiva, aumenta mutation rate!
```

**Regola:** Se unique < 4/10 â†’ popolazione troppo omogenea â†’ rischio local minimum.

**2. Fitness Standard Deviation**
```
Gen 100:  Std Dev = 45,000   âœ… Alta variabilitÃ , buona exploration
Gen 500:  Std Dev = 78,000   âœ… Eccellente
Gen 1000: Std Dev = 52,000   âœ… Ancora buona
Gen 2000: Std Dev = 15,000   âš ï¸  Troppo bassa, convergenza eccessiva
```

**Regola:** Se Std Dev < 20,000 â†’ popolazione converge troppo velocemente.

**3. Improvement Rate**
```
Gen 0-100:   +0.12 mele/gen  âœ… Apprendimento rapido
Gen 100-500: +0.08 mele/gen  âœ… Normale
Gen 500-1000: +0.03 mele/gen âš ï¸  Rallentamento
Gen 1000-2000: +0.01 mele/gen âŒ Plateau, difficile migliorare
```

### **Logs di Training (Esempio Reale)**
```
Gen    1 | Best Fit:  12547.0 | Avg Fit:   -8234.0 | Best Score:   2 | Avg Score:  0.87 | Î¼: 1.000000
Gen   10 | Best Fit:  45821.0 | Avg Fit:   12456.0 | Best Score:   5 | Avg Score:  2.13 | Î¼: 0.951229
Gen   50 | Best Fit: 187453.0 | Avg Fit:   78234.0 | Best Score:  12 | Avg Score:  6.45 | Î¼: 0.778801
Gen  100 | Best Fit: 423156.0 | Avg Fit:  198456.0 | Best Score:  18 | Avg Score: 10.23 | Î¼: 0.605006
Gen  500 | Best Fit: 1247831.0| Avg Fit:  687234.0 | Best Score:  28 | Avg Score: 17.89 | Î¼: 0.080821
Gen 1000 | Best Fit: 2145678.0| Avg Fit: 1234567.0 | Best Score:  35 | Avg Score: 22.67 | Î¼: 0.050000
```

---

## ğŸš€ Miglioramenti Futuri

### **1. Hybrid GA + Heuristics (Realizzabile)**

**Idea:** Combina GA con algoritmi classici per planning.
```python
def choose_action(state, snake_length):
    if snake_length < 20:
        # Snake corto: usa rete neurale GA
        return neural_network.predict(state)
    else:
        # Snake lungo: usa A* pathfinding sicuro
        return a_star_to_apple(snake_pos, apple_pos, body)
```

**Risultati attesi:** 60-100 mele (invece di 30-40)

### **2. Deep Q-Learning (DQN) (Avanzato)**

**Upgrade a Reinforcement Learning:**

- Experience Replay: memorizza esperienze passate
- Target Network: stabilitÃ  training
- Epsilon-greedy: balance exploration/exploitation

**Framework:** Stable-Baselines3
```bash
pip install stable-baselines3

python train_dqn.py
```

**Risultati attesi:** 100-200 mele con settimane di training

### **3. PPO (Proximal Policy Optimization) (Esperto)**

**State-of-the-art RL:**

- Policy gradient method
- PiÃ¹ stabile di DQN
- Migliori risultati su giochi

**Risultati attesi:** 150-300 mele, potenziale completamento con tuning estremo

### **4. Curriculum Learning**

**Idea:** Inizia con grid piccola, aumenta progressivamente.
```python
# Fase 1: Grid 10Ã—10 (100 celle) â†’ 30 mele con GA
# Fase 2: Grid 15Ã—15 (225 celle) â†’ transfer learning â†’ 60 mele
# Fase 3: Grid 20Ã—20 (400 celle) â†’ transfer learning â†’ 100+ mele
```

### **5. Multi-Objective Optimization**

**Fitness multi-obiettivo:**
```python
fitness = w1 * apples_eaten + 
          w2 * survival_time + 
          w3 * exploration_bonus +
          w4 * risk_avoidance
```

**Algoritmo:** NSGA-II (Non-dominated Sorting GA)

### **6. Neuroevolution (NEAT)**

**Algoritmo:** NEAT (NeuroEvolution of Augmenting Topologies)

**Differenza da GA classico:**
- Evolve sia **pesi** che **architettura** rete
- Inizia con rete semplice, aggiunge neuroni/connessioni se utili

**Risultati:** Spesso migliori di GA a paritÃ  di generazioni

---

## ğŸ“Š Confronto Finale: GA vs Altri Approcci

| Aspetto | GA (Questo) | DQN | PPO | AlphaZero |
|---------|-------------|-----|-----|-----------|
| **Score Max Atteso** | 30-50 mele | 100-200 | 150-300 | 400 (completo) |
| **FacilitÃ  Implementazione** | â­â­â­â­â­ Facile | â­â­â­ Medio | â­â­ Difficile | â­ Molto difficile |
| **Tempo Training** | Ore-Giorni | Giorni-Settimane | Settimane | Mesi |
| **Sample Efficiency** | âŒ Bassa (1M+) | âš ï¸ Media (100K) | âœ… Alta (50K) | âœ… Molto alta (10K) |
| **StabilitÃ ** | âœ… Stabile | âš ï¸ Instabile | âœ… Stabile | âœ… Molto stabile |
| **Long-term Planning** | âŒ No | âš ï¸ Limitato | âš ï¸ Limitato | âœ… Eccellente |
| **Reward Engineering** | âŒ Critico | âš ï¸ Importante | âš ï¸ Importante | âœ… Meno critico |

---

## ğŸ¤ Contribuire

Contributi benvenuti! Aree di miglioramento:

1. **Reward Function:** Sperimenta con nuove reward strategies
2. **Architettura NN:** Prova reti piÃ¹ profonde o CNN
3. **Hyperparameter Tuning:** Ottimizza mutation rate, population size, ecc.
4. **Visualization:** Dashboard real-time con matplotlib/plotly
5. **Benchmarking:** Confronta con implementazioni DQN

**Come contribuire:**
```bash
# 1. Fork repository
# 2. Crea branch feature
git checkout -b feature/nome-feature

# 3. Commit modifiche
git commit -m "feat: descrizione"

# 4. Push e apri Pull Request
git push origin feature/nome-feature
```

---

## ğŸ™ Ringraziamenti

- **Algoritmi Genetici:** Ispirazione da paper classici di John Holland
- **Snake Game:** Basato su implementazione pygame classica
- **Neural Networks:** Architettura ispirata da NeuroEvolution papers

---

## ğŸ“¬ Contatti

- **GitHub:** [Eddicpp](https://github.com/Eddicpp)
- **Email:** eduardo.pane04@gmail.com

---

**Buon training! ğŸğŸ§¬ğŸš€**
